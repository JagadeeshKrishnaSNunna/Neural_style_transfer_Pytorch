{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b31f3ee4-d047-4a40-8e91-280a4376af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d379bb-4ded-4ba3-8ae1-7db05a0cd1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f9f977-5943-403e-a9e2-54584fb6093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jagadeesh/Desktop/workspace/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jagadeesh/Desktop/workspace/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg19(pretrained=True).features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc2aae1-8f69-4e91-8c47-3b7f23179c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (17): ReLU(inplace=True)\n",
      "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (24): ReLU(inplace=True)\n",
      "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (26): ReLU(inplace=True)\n",
      "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (31): ReLU(inplace=True)\n",
      "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (33): ReLU(inplace=True)\n",
      "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (35): ReLU(inplace=True)\n",
      "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df18734-1c80-41a6-bdd8-abe14a69a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['0','5','10','19','28'] are the interested layers and not interested in layers after 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d5288f-5cf1-469a-aa7d-66b1f6c81b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(VGG,self).__init__()\n",
    "    self.choosen_features=['0','5','10','19','28']\n",
    "    self.model=models.vgg19(pretrained=True).features[:29]\n",
    "  def forward(self,x):\n",
    "    features=[]\n",
    "    for layer_num,layer in enumerate(self.model):\n",
    "      x=layer(x)\n",
    "      if str(layer_num) in self.choosen_features:\n",
    "        features.append(x)\n",
    "    return(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ad7c92-d37b-4586-917c-76515895be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=VGG().to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c93bdf-98b2-4fbe-83ec-e8d2e8891b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=512\n",
    "loader=transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e030733-7f55-4d9a-9dfd-b4424ce98bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "  image=Image.open(image_path).convert('RGB')\n",
    "  image=loader(image).unsqueeze(0)\n",
    "  return image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d28ed8e8-6106-4e69-9230-759a12088136",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image=load_image('content.png')\n",
    "style_image=load_image('style1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3b22c40-85c3-4322-8310-5c26b2078064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_image=torch.randn(original_image.shape,device=device,requires_grad=True)\n",
    "generated_image=original_image.clone().requires_grad_(True) #gives better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89c96c6f-6f63-4a6a-8ae9-02c357ed4846",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps=2000\n",
    "learning_rate=0.003\n",
    "# alpha=1\n",
    "# beta=90\n",
    "alpha=1\n",
    "beta=0.001\n",
    "optimizer=optim.Adam([generated_image],lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ba7d1-d6a9-4303-a0e6-5c51fd3e4bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(316617.2500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(6467.0444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2372.6353, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1357.5688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(954.9714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(736.1094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(595.2604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(496.4590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(429.7458, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(371.3081, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for step in range(total_steps):\n",
    "  generated_features=model(generated_image)\n",
    "  original_features=model(original_image)\n",
    "  style_features=model(style_image)\n",
    "  style_loss = 0\n",
    "  original_loss = 0\n",
    "    \n",
    "  for gen_feature,orig_feature,style_feature in zip(generated_features,original_features,style_features):\n",
    "    batch_size,channel,height,width=gen_feature.shape\n",
    "    original_loss+=torch.mean((gen_feature-orig_feature)**2)\n",
    "\n",
    "    #Gram Matrix\n",
    "    G=gen_feature.view(channel,height*width).mm(gen_feature.view(channel,height*width).t())\n",
    "    A=style_feature.view(channel,height*width).mm(style_feature.view(channel,height*width).t())\n",
    "\n",
    "    style_loss+=torch.mean((G-A)**2)\n",
    "  total_loss=alpha*original_loss+beta*style_loss\n",
    "  optimizer.zero_grad()\n",
    "  total_loss.backward()\n",
    "  optimizer.step()\n",
    "  if step%200==0:\n",
    "    print(total_loss)\n",
    "  if step==(total_steps-1):\n",
    "    save_image(generated_image,\"generated\"+str(step)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d16afc-233e-4de6-bb63-df0719471454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
